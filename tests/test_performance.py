"""
Performance tests for the Document Search & Retrieval System.

Tests verify that the system meets performance requirements:
- Search latency <3s (p95)
- Document processing <30s per document
- Concurrent upload handling (10 simultaneous)
"""

import pytest
import time
import io
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from unittest.mock import Mock, patch

from fastapi.testclient import TestClient
from src.main import app
from src.config import settings


client = TestClient(app)


@pytest.mark.performance
@pytest.mark.slow
class TestSearchPerformance:
    """Test search performance requirements."""

    @patch("src.api.search.get_search_service")
    def test_search_latency_under_3_seconds(self, mock_search_service):
        """Test that search responds in <3 seconds (p95)."""
        # Mock search service with realistic delay
        mock_service = Mock()
        mock_service.search.return_value = {
            "took": 50,
            "total": 10,
            "results": [
                {
                    "document_id": f"doc-{i}",
                    "filename": f"test-{i}.pdf",
                    "page": 1,
                    "score": 0.9,
                    "snippet": "test content",
                    "summary": "test summary"
                }
                for i in range(10)
            ]
        }
        mock_search_service.return_value = mock_service

        # Perform 100 searches to get p95
        latencies = []

        for i in range(100):
            start_time = time.time()

            response = client.post(
                "/api/v1/search",
                json={
                    "query": f"test query {i}",
                    "page": 1,
                    "page_size": 10
                }
            )

            latency = time.time() - start_time
            latencies.append(latency)

            assert response.status_code == 200

        # Calculate p95 (95th percentile)
        latencies.sort()
        p95_index = int(len(latencies) * 0.95)
        p95_latency = latencies[p95_index]

        print(f"\nSearch Performance:")
        print(f"  Min: {min(latencies):.3f}s")
        print(f"  Median: {latencies[len(latencies)//2]:.3f}s")
        print(f"  p95: {p95_latency:.3f}s")
        print(f"  Max: {max(latencies):.3f}s")

        # Assert p95 < 3 seconds
        assert p95_latency < 3.0, f"p95 latency {p95_latency:.3f}s exceeds 3s target"

    @patch("src.api.search.get_search_service")
    def test_search_response_time_avg(self, mock_search_service):
        """Test average search response time."""
        mock_service = Mock()
        mock_service.search.return_value = {
            "took": 50,
            "total": 5,
            "results": []
        }
        mock_search_service.return_value = mock_service

        # Perform 50 searches
        total_time = 0
        num_searches = 50

        for i in range(num_searches):
            start_time = time.time()

            response = client.post(
                "/api/v1/search",
                json={"query": f"test {i}"}
            )

            total_time += time.time() - start_time
            assert response.status_code == 200

        avg_time = total_time / num_searches
        print(f"\nAverage search time: {avg_time:.3f}s")

        # Average should be well under 1 second
        assert avg_time < 1.0, f"Average search time {avg_time:.3f}s is too slow"


@pytest.mark.performance
@pytest.mark.slow
class TestDocumentProcessingPerformance:
    """Test document processing performance."""

    @patch("src.api.documents.DocumentProcessor")
    @patch("src.api.documents.get_postgres_client")
    def test_document_processing_time(
        self, mock_pg_client, mock_processor_class
    ):
        """Test that document processing completes in <30 seconds."""
        # Mock processor to simulate realistic processing time
        mock_processor = Mock()

        # Simulate processing stages with realistic delays
        def mock_process(*args, **kwargs):
            time.sleep(0.1)  # Simulate processing time
            return {
                "document_id": "test-doc",
                "status": "ready",
                "total_pages": 5,
                "pages_indexed": 5,
                "summaries_generated": 5,
                "error_message": None
            }

        mock_processor.process_document = Mock(side_effect=mock_process)
        mock_processor_class.return_value = mock_processor

        # Mock database
        mock_pg_client.return_value.create_document.return_value = Mock()

        # Create sample PDF
        pdf_content = b"%PDF-1.4\n%Test PDF\n%%EOF"
        files = {"file": ("test.pdf", io.BytesIO(pdf_content), "application/pdf")}
        data = {"category": "maintenance", "machine_model": "TEST-100"}

        # Measure upload + processing time
        start_time = time.time()

        response = client.post(
            "/api/v1/documents/upload",
            files=files,
            data=data,
            headers={"Authorization": f"Bearer {settings.api_key}"}
        )

        # Wait for background task to complete (simplified for test)
        time.sleep(0.2)

        processing_time = time.time() - start_time

        print(f"\nDocument processing time: {processing_time:.3f}s")

        assert response.status_code == 202
        assert processing_time < 30.0, f"Processing time {processing_time:.3f}s exceeds 30s target"


@pytest.mark.performance
@pytest.mark.slow
class TestConcurrentUploads:
    """Test concurrent upload handling."""

    @patch("src.api.documents.DocumentProcessor")
    @patch("src.api.documents.get_postgres_client")
    def test_concurrent_upload_handling(
        self, mock_pg_client, mock_processor_class
    ):
        """Test system handles 10 simultaneous uploads."""
        # Mock processor
        mock_processor = Mock()
        mock_processor.process_document = Mock(return_value={
            "document_id": "test-doc",
            "status": "ready",
            "total_pages": 1
        })
        mock_processor_class.return_value = mock_processor

        # Mock database
        doc_counter = 0

        def create_doc(*args, **kwargs):
            nonlocal doc_counter
            doc_counter += 1
            return Mock(id=f"doc-{doc_counter}")

        mock_pg_client.return_value.create_document = Mock(side_effect=create_doc)

        # Prepare 10 uploads
        def upload_document(index: int) -> dict:
            """Upload a single document."""
            pdf_content = b"%PDF-1.4\n%Test PDF\n%%EOF"
            files = {
                "file": (f"test_{index}.pdf", io.BytesIO(pdf_content), "application/pdf")
            }
            data = {
                "category": "maintenance",
                "machine_model": f"MODEL-{index}"
            }

            start_time = time.time()

            response = client.post(
                "/api/v1/documents/upload",
                files=files,
                data=data,
                headers={"Authorization": f"Bearer {settings.api_key}"}
            )

            upload_time = time.time() - start_time

            return {
                "index": index,
                "status_code": response.status_code,
                "upload_time": upload_time,
                "response": response.json() if response.status_code == 202 else None
            }

        # Execute concurrent uploads
        start_time = time.time()

        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = [executor.submit(upload_document, i) for i in range(10)]
            results = [future.result() for future in as_completed(futures)]

        total_time = time.time() - start_time

        # Verify all uploads succeeded
        success_count = sum(1 for r in results if r["status_code"] == 202)
        upload_times = [r["upload_time"] for r in results]

        print(f"\nConcurrent Upload Test (10 simultaneous):")
        print(f"  Total time: {total_time:.3f}s")
        print(f"  Successful uploads: {success_count}/10")
        print(f"  Avg upload time: {sum(upload_times)/len(upload_times):.3f}s")
        print(f"  Max upload time: {max(upload_times):.3f}s")

        # All uploads should succeed
        assert success_count == 10, f"Only {success_count}/10 uploads succeeded"

        # Total time should be reasonable (not 10x single upload time)
        # With parallelization, should be <5x single upload time
        assert total_time < 5.0, f"Concurrent uploads took {total_time:.3f}s (too slow)"

    @patch("src.api.documents.DocumentProcessor")
    @patch("src.api.documents.get_postgres_client")
    def test_concurrent_upload_no_conflicts(
        self, mock_pg_client, mock_processor_class
    ):
        """Test that concurrent uploads don't cause conflicts."""
        # Mock processor
        mock_processor = Mock()
        mock_processor.process_document = Mock(return_value={
            "status": "ready"
        })
        mock_processor_class.return_value = mock_processor

        # Track created documents
        created_docs = []

        def create_doc(*args, **kwargs):
            import uuid
            doc_id = str(uuid.uuid4())
            created_docs.append(doc_id)
            return Mock(id=doc_id)

        mock_pg_client.return_value.create_document = Mock(side_effect=create_doc)

        # Upload 20 documents concurrently
        def upload(i):
            pdf_content = b"%PDF-1.4\n%Test\n%%EOF"
            files = {"file": (f"test_{i}.pdf", io.BytesIO(pdf_content), "application/pdf")}
            data = {"category": "maintenance"}

            response = client.post(
                "/api/v1/documents/upload",
                files=files,
                data=data,
                headers={"Authorization": f"Bearer {settings.api_key}"}
            )

            if response.status_code == 202:
                return response.json()["document_id"]
            return None

        with ThreadPoolExecutor(max_workers=20) as executor:
            futures = [executor.submit(upload, i) for i in range(20)]
            document_ids = [f.result() for f in as_completed(futures)]

        # Remove None values
        document_ids = [d for d in document_ids if d]

        # All document IDs should be unique (no conflicts)
        unique_ids = set(document_ids)
        print(f"\nConcurrent uploads: {len(document_ids)} total, {len(unique_ids)} unique")

        assert len(unique_ids) == len(document_ids), "Document ID conflicts detected"
        assert len(unique_ids) == 20, f"Expected 20 unique IDs, got {len(unique_ids)}"


@pytest.mark.performance
class TestSearchScalability:
    """Test search performance with varying result set sizes."""

    @patch("src.api.search.get_search_service")
    def test_search_with_large_result_set(self, mock_search_service):
        """Test search performance with 1000 results."""
        # Mock large result set
        mock_service = Mock()
        mock_service.search.return_value = {
            "took": 100,
            "total": 1000,
            "results": [
                {
                    "document_id": f"doc-{i}",
                    "filename": f"test-{i}.pdf",
                    "page": 1,
                    "score": 0.8,
                    "snippet": "test" * 50,  # ~200 chars
                    "summary": "summary" * 30  # ~210 chars
                }
                for i in range(100)  # First page of 100 results
            ]
        }
        mock_search_service.return_value = mock_service

        start_time = time.time()

        response = client.post(
            "/api/v1/search",
            json={
                "query": "test",
                "page": 1,
                "page_size": 100
            }
        )

        elapsed_time = time.time() - start_time

        print(f"\nLarge result set search time: {elapsed_time:.3f}s")

        assert response.status_code == 200
        assert elapsed_time < 2.0, f"Large result set search took {elapsed_time:.3f}s"

        data = response.json()
        assert data["total"] == 1000
        assert len(data["results"]) == 100


@pytest.mark.performance
class TestMemoryUsage:
    """Test memory usage during operations."""

    @patch("src.api.documents.DocumentProcessor")
    @patch("src.api.documents.get_postgres_client")
    def test_upload_memory_efficiency(
        self, mock_pg_client, mock_processor_class
    ):
        """Test that large file uploads don't cause memory issues."""
        # Mock processor and database
        mock_processor = Mock()
        mock_processor.process_document = Mock(return_value={"status": "ready"})
        mock_processor_class.return_value = mock_processor

        mock_pg_client.return_value.create_document = Mock(
            return_value=Mock(id="test-doc")
        )

        # Create a large PDF (10MB)
        pdf_header = b"%PDF-1.4\n"
        pdf_content = pdf_header + (b"A" * (10 * 1024 * 1024))  # 10MB
        pdf_footer = b"\n%%EOF"
        large_pdf = pdf_content + pdf_footer

        files = {"file": ("large.pdf", io.BytesIO(large_pdf), "application/pdf")}
        data = {"category": "maintenance"}

        # This should not cause memory issues (streaming upload)
        response = client.post(
            "/api/v1/documents/upload",
            files=files,
            data=data,
            headers={"Authorization": f"Bearer {settings.api_key}"}
        )

        assert response.status_code in [202, 413]  # 413 if exceeds size limit

        # If accepted, verify it was handled correctly
        if response.status_code == 202:
            print("\n10MB file uploaded successfully without memory issues")
